{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b54b5f74-b4c3-403e-a3f6-967e19f4594b",
   "metadata": {},
   "source": [
    "THIS IS AN EXPERIMENTAL FILE AND NOT PART OF SUBMISSION!!! IT HAS FUNCTIONAL AND NON FUNCTIONAL EXPEREMENTAL VERSION OF INFERENC SCRIPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88e0981-0387-459f-bc2d-126a0bafcb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "             aspect_ratio: 1.0                           \n",
      "               batch_size: 1                             \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "                crop_size: 256                           \n",
      "                 dataroot: ./my_dataset                  \t[default: None]\n",
      "             dataset_mode: unaligned                     \n",
      "                direction: AtoB                          \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "                     eval: False                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: False                         \t[default: None]\n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 256                           \n",
      "         max_dataset_size: inf                           \n",
      "                    model: cycle_gan                     \t[default: test]\n",
      "               n_layers_D: 3                             \n",
      "                     name: human2arcane                  \t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "               no_dropout: True                          \n",
      "                  no_flip: False                         \n",
      "                     norm: instance                      \n",
      "                 num_test: 1                             \t[default: 50]\n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: test                          \n",
      "               preprocess: resize_and_crop               \n",
      "              results_dir: ./results/                    \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "                use_wandb: False                         \n",
      "                  verbose: False                         \n",
      "       wandb_project_name: CycleGAN-and-pix2pix          \n",
      "----------------- End -------------------\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [CycleGANModel] was created\n",
      "loading the model from ./checkpoints\\human2arcane\\latest_net_G_A.pth\n",
      "loading the model from ./checkpoints\\human2arcane\\latest_net_G_B.pth\n",
      "---------- Networks initialized -------------\n",
      "[Network G_A] Total number of parameters : 11.378 M\n",
      "[Network G_B] Total number of parameters : 11.378 M\n",
      "-----------------------------------------------\n",
      "Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from options.test_options import TestOptions\n",
    "from models import create_model\n",
    "\n",
    "# ── 1) Parse TestOptions via sys.argv ─────────────────────────────────────────\n",
    "sys.argv = [\n",
    "    'test.py',\n",
    "    '--dataroot',   './my_dataset',\n",
    "    '--name',       'human2arcane',\n",
    "    '--model',      'cycle_gan',\n",
    "    '--phase',      'test',\n",
    "    '--num_test',   '1',\n",
    "    '--load_size',  '256',\n",
    "    '--crop_size',  '256',\n",
    "]\n",
    "opt = TestOptions().parse()\n",
    "model = create_model(opt)\n",
    "model.setup(opt)\n",
    "model.eval()\n",
    "\n",
    "# ── 2) Build full-frame preprocess transform (match test.py) ─────────────────\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize to model input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# ── 3) Start capture ──────────────────────────────────────────────────────────\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert frame to PIL and preprocess\n",
    "    img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    input_tensor = preprocess(img_pil).unsqueeze(0).to(model.device)\n",
    "\n",
    "    # Stylize full frame\n",
    "    with torch.no_grad():\n",
    "        fake = model.netG_A(input_tensor)\n",
    "\n",
    "    # Convert back to uint8 image\n",
    "    fake_np = fake.squeeze().cpu().permute(1, 2, 0).numpy()  # HxWxC in [-1,1]\n",
    "    fake_img = ((fake_np + 1) / 2 * 255).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    # Resize stylized image back to original frame size\n",
    "    fake_bgr = cv2.cvtColor(fake_img, cv2.COLOR_RGB2BGR)\n",
    "    output = cv2.resize(fake_bgr, (frame.shape[1], frame.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # Show result\n",
    "    cv2.imshow('Human 2 Arcane - Full Frame', output)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1657b36-0dbe-4b32-bc39-6376b019281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is for webcam\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch # For torch.no_grad()\n",
    "from PIL import Image, ImageEnhance\n",
    "import torchvision.transforms as transforms\n",
    "# Ensure these custom modules are in your Python path or the same directory\n",
    "# from options.test_options import TestOptions\n",
    "# from models import create_model\n",
    "import pyvirtualcam\n",
    "\n",
    "# --- Fallback Mock Model (if your CycleGAN classes aren't found) ---\n",
    "try:\n",
    "    from options.test_options import TestOptions\n",
    "    from models import create_model\n",
    "    MODEL_CLASSES_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"WARNING: 'options.test_options' or 'models.create_model' not found.\")\n",
    "    print(\"Using a MOCK model. Output will not be style-transferred.\")\n",
    "    MODEL_CLASSES_AVAILABLE = False\n",
    "\n",
    "    class MockTestOptions:\n",
    "        def __init__(self):\n",
    "            self.gpu_ids = []\n",
    "            self.isTrain = False\n",
    "            self.dataroot = './my_dataset'\n",
    "            self.name = 'human2arcane_mock'\n",
    "            self.model = 'cycle_gan'\n",
    "            self.phase = 'test'\n",
    "            self.num_test = 1\n",
    "            self.load_size = 256\n",
    "            self.crop_size = 256\n",
    "            self.no_flip = True\n",
    "            self.direction = 'AtoB'\n",
    "            self.dataset_mode = 'unaligned'\n",
    "            self.serial_batches = True\n",
    "            self.num_threads = 0\n",
    "            self.batch_size = 1\n",
    "            self.max_dataset_size = float(\"inf\")\n",
    "            self.input_nc = 3\n",
    "            self.output_nc = 3\n",
    "            self.ngf = 64\n",
    "            self.ndf = 64\n",
    "            self.netG = 'resnet_9blocks'\n",
    "            self.netD = 'basic'\n",
    "            self.norm = 'instance'\n",
    "            self.init_type = 'normal'\n",
    "            self.init_gain = 0.02\n",
    "            self.no_dropout = True\n",
    "            self.verbose = False\n",
    "            self.preprocess = 'resize_and_crop'\n",
    "\n",
    "        def parse(self):\n",
    "            print(\"MockTestOptions parsed.\")\n",
    "            self.gpu_ids = [] if not torch.cuda.is_available() else [0]\n",
    "            return self\n",
    "\n",
    "    class MockModel:\n",
    "        def __init__(self, opt):\n",
    "            self.opt = opt\n",
    "            self.device = torch.device('cuda:{}'.format(opt.gpu_ids[0])) if opt.gpu_ids and torch.cuda.is_available() else torch.device('cpu')\n",
    "            print(f\"MockModel using device: {self.device}\")\n",
    "            self.netG_A = lambda x: torch.clamp(x * 0.8, -1.0, 1.0)\n",
    "            self.netG = self.netG_A\n",
    "\n",
    "        def setup(self, opt):\n",
    "            print(f\"MockModel setup with options: {opt.name}\")\n",
    "        def eval(self):\n",
    "            print(\"MockModel set to eval mode.\")\n",
    "        def set_input(self, input_data):\n",
    "            self.real_A = input_data['A'].to(self.device)\n",
    "            # Mock doesn't strictly need B for test, but real model might\n",
    "            if 'B' in input_data:\n",
    "                self.real_B = input_data['B'].to(self.device)\n",
    "            self.image_paths = input_data.get('A_paths', [])\n",
    "        def test(self):\n",
    "            with torch.no_grad():\n",
    "                self.fake_B = self.netG_A(self.real_A)\n",
    "        def get_current_visuals(self):\n",
    "            visual_ret = {}\n",
    "            if hasattr(self, 'real_A'): visual_ret['real_A'] = self.real_A\n",
    "            if hasattr(self, 'fake_B'): visual_ret['fake_B'] = self.fake_B\n",
    "            return visual_ret\n",
    "\n",
    "# --- 1) Parse Options and Load Model ---\n",
    "print(\"Initializing CycleGAN model...\")\n",
    "if MODEL_CLASSES_AVAILABLE:\n",
    "    sys.argv = [\n",
    "        'test.py',\n",
    "        '--dataroot', './my_dataset',\n",
    "        '--name', 'human2arcane',\n",
    "        '--model', 'cycle_gan',\n",
    "        '--phase', 'test',\n",
    "        '--num_test', '1',\n",
    "        '--load_size', '256',\n",
    "        '--crop_size', '256',\n",
    "        '--no_flip',\n",
    "        # Ensure your command line options for testing are appropriate\n",
    "        # For example, some models might expect --dataset_mode unaligned even for single image test\n",
    "        # or specific --netG, --norm options if not default in your saved model.\n",
    "        # Usually, the options saved with the model during training are loaded.\n",
    "    ]\n",
    "    opt = TestOptions().parse()\n",
    "    if hasattr(opt, 'isTrain'): opt.isTrain = False\n",
    "    model = create_model(opt)\n",
    "    model.setup(opt)\n",
    "    model.eval()\n",
    "    model_device = model.device\n",
    "else:\n",
    "    opt = MockTestOptions().parse()\n",
    "    model = MockModel(opt)\n",
    "    model.setup(opt)\n",
    "    model.eval()\n",
    "    model_device = model.device\n",
    "print(f\"Model '{opt.name}' loaded. Using device: {model_device}\")\n",
    "\n",
    "# --- 2) Preprocessing for CycleGAN ---\n",
    "preprocess_transform = transforms.Compose([\n",
    "    transforms.Resize((opt.crop_size, opt.crop_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# --- 3) Webcam Capture ---\n",
    "WEBCAM_INDEX = 0\n",
    "FPS_TARGET = 30\n",
    "cap = cv2.VideoCapture(WEBCAM_INDEX)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(f\"❌ Could not open webcam at index {WEBCAM_INDEX}.\")\n",
    "cap.set(cv2.CAP_PROP_FPS, FPS_TARGET)\n",
    "actual_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "print(f\"Webcam actual FPS: {actual_fps if actual_fps > 0 else 'Could not get FPS, using target.'}\")\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    cap.release()\n",
    "    raise RuntimeError(\"❌ Failed to read initial frame from webcam.\")\n",
    "webcam_h, webcam_w = frame.shape[:2]\n",
    "print(f\"Webcam opened successfully: {webcam_w}x{webcam_h}\")\n",
    "\n",
    "# --- 4) Create Separate OpenCV Windows and Trackbars ---\n",
    "CONTROLS_WINDOW_NAME = \"Adjustments\"\n",
    "PREVIEW_WINDOW_NAME = \"CycleGAN Virtual Camera\"\n",
    "cv2.namedWindow(CONTROLS_WINDOW_NAME)\n",
    "cv2.createTrackbar(\"Brightness\", CONTROLS_WINDOW_NAME, 100, 200, lambda x: None)\n",
    "cv2.createTrackbar(\"Contrast\", CONTROLS_WINDOW_NAME, 100, 200, lambda x: None)\n",
    "cv2.namedWindow(PREVIEW_WINDOW_NAME)\n",
    "\n",
    "# --- 5) Start OBS VirtualCam ---\n",
    "try:\n",
    "    print(f\"Attempting to initialize pyvirtualcam with: width={webcam_w}, height={webcam_h}, fps={FPS_TARGET}\")\n",
    "    with pyvirtualcam.Camera(width=webcam_w, height=webcam_h, fps=FPS_TARGET, backend='obs', print_fps=True) as cam:\n",
    "        print(f\"✅ Virtual Camera started: {cam.device} ({cam.width}x{cam.height} @ {cam.fps} FPS)\")\n",
    "        print(\"📸 Press 'q' in the preview window or Ctrl+C in console to quit.\")\n",
    "\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                print(\"❌ Failed to grab frame from webcam. Exiting loop.\")\n",
    "                break\n",
    "            brightness_val = cv2.getTrackbarPos(\"Brightness\", CONTROLS_WINDOW_NAME)\n",
    "            contrast_val = cv2.getTrackbarPos(\"Contrast\", CONTROLS_WINDOW_NAME)\n",
    "            brightness_factor = brightness_val / 100.0\n",
    "            contrast_factor = contrast_val / 100.0\n",
    "            frame_rgb_pil = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            img_pil = Image.fromarray(frame_rgb_pil)\n",
    "            enhancer_brightness = ImageEnhance.Brightness(img_pil)\n",
    "            img_pil_enhanced = enhancer_brightness.enhance(brightness_factor)\n",
    "            enhancer_contrast = ImageEnhance.Contrast(img_pil_enhanced)\n",
    "            img_pil_final_for_gan = enhancer_contrast.enhance(contrast_factor)\n",
    "            input_tensor = preprocess_transform(img_pil_final_for_gan).unsqueeze(0).to(model_device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if MODEL_CLASSES_AVAILABLE and hasattr(model, 'set_input') and hasattr(model, 'test') and hasattr(model, 'get_current_visuals'):\n",
    "                    # Standard Jun-Yan Zhu's repo structure\n",
    "                    input_dict_for_set_input = {\n",
    "                        'A': input_tensor,\n",
    "                        'A_paths': ['dummy_A_path'], # Corresponds to input['A']\n",
    "                        # Provide dummy 'B' and 'B_paths' for CycleGANModel.set_input\n",
    "                        'B': input_tensor.clone().detach(), # Dummy B, content doesn't matter for netG_A(real_A)\n",
    "                        'B_paths': ['dummy_B_path']  # Dummy B_paths for input['B']\n",
    "                    }\n",
    "                    model.set_input(input_dict_for_set_input)\n",
    "                    model.test()\n",
    "                    visuals = model.get_current_visuals()\n",
    "                    fake_output_tensor = visuals.get('fake_B', visuals.get('fake', input_tensor))\n",
    "                elif hasattr(model, 'netG_A'):\n",
    "                     fake_output_tensor = model.netG_A(input_tensor)\n",
    "                elif hasattr(model, 'netG'):\n",
    "                     fake_output_tensor = model.netG(input_tensor)\n",
    "                else:\n",
    "                    print(\"WARNING: Could not determine model inference method. Passing through input.\")\n",
    "                    fake_output_tensor = input_tensor\n",
    "\n",
    "            fake_np = fake_output_tensor.squeeze(0).cpu().permute(1, 2, 0).numpy()\n",
    "            output_img_normalized = (fake_np + 1) / 2.0\n",
    "            output_img_uint8 = (output_img_normalized * 255.0).clip(0, 255).astype(np.uint8)\n",
    "            if output_img_uint8.ndim == 2:\n",
    "                output_img_uint8 = cv2.cvtColor(output_img_uint8, cv2.COLOR_GRAY2RGB)\n",
    "            elif output_img_uint8.shape[2] == 1:\n",
    "                output_img_uint8 = cv2.cvtColor(output_img_uint8, cv2.COLOR_GRAY2RGB)\n",
    "            if output_img_uint8.shape[0] != webcam_h or output_img_uint8.shape[1] != webcam_w:\n",
    "                output_final_rgb = cv2.resize(output_img_uint8, (webcam_w, webcam_h), interpolation=cv2.INTER_LINEAR)\n",
    "            else:\n",
    "                output_final_rgb = output_img_uint8\n",
    "            preview_display_bgr = cv2.cvtColor(output_final_rgb, cv2.COLOR_RGB2BGR)\n",
    "            cv2.imshow(PREVIEW_WINDOW_NAME, preview_display_bgr)\n",
    "            cv2.imshow(CONTROLS_WINDOW_NAME, frame_bgr)\n",
    "            cam.send(output_final_rgb)\n",
    "            cam.sleep_until_next_frame()\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                print(\"INFO: 'q' pressed, exiting.\")\n",
    "                break\n",
    "            try:\n",
    "                if cv2.getWindowProperty(PREVIEW_WINDOW_NAME, cv2.WND_PROP_VISIBLE) < 1 or \\\n",
    "                   cv2.getWindowProperty(CONTROLS_WINDOW_NAME, cv2.WND_PROP_VISIBLE) < 1:\n",
    "                    print(\"INFO: A window was closed, exiting.\")\n",
    "                    break\n",
    "            except cv2.error:\n",
    "                print(\"INFO: OpenCV window error, likely closed. Exiting.\")\n",
    "                break\n",
    "except KeyboardInterrupt:\n",
    "    print(\"👋 Exiting due to KeyboardInterrupt...\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"🔥 PYVIRTUALCAM RUNTIME ERROR: {e}\")\n",
    "    print(\"   Ensure OBS Studio is running (AS ADMINISTRATOR if issues persist) and the Virtual Camera is started in OBS,\")\n",
    "    print(\"   or that the chosen backend and camera parameters (resolution/FPS) are correctly configured and match OBS settings.\")\n",
    "except Exception as e:\n",
    "    print(f\"💥 An unexpected error occurred: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    print(\"Releasing resources...\")\n",
    "    if 'cap' in locals() and cap.isOpened():\n",
    "        cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Cleanup finished. Exiting application.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1283919-310b-4dfa-855c-3bda3cb5d511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from options.test_options import TestOptions\n",
    "from models import create_model\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "# ── 1) Parse TestOptions via sys.argv ─────────────────────────────────────────\n",
    "sys.argv = [\n",
    "    'test.py',\n",
    "    '--dataroot',   './my_dataset',\n",
    "    '--name',       'human2arcane',\n",
    "    '--model',      'cycle_gan',\n",
    "    '--phase',      'test',\n",
    "    '--num_test',   '1',\n",
    "    '--load_size',  '256',\n",
    "    '--crop_size',  '256',\n",
    "]\n",
    "opt = TestOptions().parse()\n",
    "model = create_model(opt)\n",
    "model.setup(opt)\n",
    "model.eval()\n",
    "\n",
    "# ── 2) Build preprocessing transform ──────────────────────────────────────────\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# ── 3) Prepare face detector ──────────────────────────────────────────────────\n",
    "mtcnn = MTCNN(keep_all=False)\n",
    "\n",
    "# ── 4) Start webcam capture ───────────────────────────────────────────────────\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    boxes, _ = mtcnn.detect(img_pil)\n",
    "    output = frame.copy()\n",
    "\n",
    "    if boxes is not None and len(boxes):\n",
    "        h, w = frame.shape[:2]\n",
    "\n",
    "        # Original bounding box\n",
    "        x1 = int(boxes[0][0])\n",
    "        y1 = int(boxes[0][1])\n",
    "        x2 = int(boxes[0][2])\n",
    "        y2 = int(boxes[0][3])\n",
    "\n",
    "        # Padding percentage\n",
    "        padding = 0.3  # 30% padding on each side\n",
    "        w_box = x2 - x1\n",
    "        h_box = y2 - y1\n",
    "\n",
    "        # Expand bounding box by padding percentage\n",
    "        x1 = max(0, x1 - int(w_box * padding))\n",
    "        y1 = max(0, y1 - int(h_box * padding))\n",
    "        x2 = min(w, x2 + int(w_box * padding))\n",
    "        y2 = min(h, y2 + int(h_box * padding))\n",
    "\n",
    "        # Final size after expansion\n",
    "        w_box = x2 - x1\n",
    "        h_box = y2 - y1\n",
    "\n",
    "        if w_box > 0 and h_box > 0:\n",
    "            # Crop & preprocess\n",
    "            face_crop = img_pil.crop((x1, y1, x2, y2))\n",
    "            face_tensor = preprocess(face_crop).unsqueeze(0).to(model.device)\n",
    "\n",
    "            # Stylize\n",
    "            with torch.no_grad():\n",
    "                fake = model.netG_A(face_tensor)\n",
    "\n",
    "            # Convert back to uint8 image\n",
    "            fake_np = fake.squeeze().cpu().permute(1, 2, 0).numpy()  # HxWxC in [-1,1]\n",
    "            fake_img = ((fake_np + 1) / 2 * 255).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "            # Resize and paste\n",
    "            fake_resized = cv2.resize(fake_img, (w_box, h_box), interpolation=cv2.INTER_LINEAR)\n",
    "            output[y1:y2, x1:x2] = fake_resized\n",
    "\n",
    "    cv2.imshow('Human 2 Arcane Live', output)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3486de-3284-4d68-89fc-26281e72eb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from options.test_options import TestOptions\n",
    "from models import create_model\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "# ── 1) Parse TestOptions via sys.argv ─────────────────────────────────────────\n",
    "sys.argv = [\n",
    "    'test.py',\n",
    "    '--dataroot',   './my_dataset',\n",
    "    '--name',       'human2arcane',\n",
    "    '--model',      'cycle_gan',\n",
    "    '--phase',      'test',\n",
    "    '--num_test',   '1',\n",
    "    '--load_size',  '256',\n",
    "    '--crop_size',  '256',\n",
    "]\n",
    "opt = TestOptions().parse()\n",
    "model = create_model(opt)\n",
    "model.setup(opt)\n",
    "model.eval()\n",
    "\n",
    "# ── 2) Image preprocessing transform ─────────────────────────────────────────\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# ── 3) Face detector ─────────────────────────────────────────────────────────\n",
    "mtcnn = MTCNN(keep_all=False)\n",
    "\n",
    "# ── 4) Start webcam capture ──────────────────────────────────────────────────\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"Press 'q' to quit.\")\n",
    "\n",
    "# Define percentage margin around detected face\n",
    "MARGIN_PCT = 0.3  # 30% expansion in all directions\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    boxes, _ = mtcnn.detect(img_pil)\n",
    "    output = frame.copy()\n",
    "\n",
    "    if boxes is not None and len(boxes):\n",
    "        h, w = frame.shape[:2]\n",
    "        x1, y1, x2, y2 = boxes[0]\n",
    "\n",
    "        # Expand crop with margin\n",
    "        box_w, box_h = x2 - x1, y2 - y1\n",
    "        margin_w = box_w * MARGIN_PCT\n",
    "        margin_h = box_h * MARGIN_PCT\n",
    "\n",
    "        # Apply margin and clip to image boundaries\n",
    "        x1 = int(max(0, x1 - margin_w))\n",
    "        y1 = int(max(0, y1 - margin_h))\n",
    "        x2 = int(min(w, x2 + margin_w))\n",
    "        y2 = int(min(h, y2 + margin_h))\n",
    "\n",
    "        # Crop, preprocess and stylize\n",
    "        face_crop = img_pil.crop((x1, y1, x2, y2))\n",
    "        face_tensor = preprocess(face_crop).unsqueeze(0).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            fake = model.netG_A(face_tensor)\n",
    "\n",
    "        # Convert to uint8 RGB image\n",
    "        fake_np = fake.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "        fake_img = ((fake_np + 1) / 2 * 255).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "        # Resize to match original crop and convert to BGR for OpenCV\n",
    "        w_box, h_box = x2 - x1, y2 - y1\n",
    "        fake_resized = cv2.resize(fake_img, (w_box, h_box), interpolation=cv2.INTER_LINEAR)\n",
    "        fake_resized = cv2.cvtColor(fake_resized, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Paste back into frame\n",
    "        output[y1:y2, x1:x2] = fake_resized\n",
    "\n",
    "    # Show result\n",
    "    cv2.imshow('Human 2 Arcane Live', output)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a3a896-f7c4-4aa6-bd45-de19ba35e067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from options.test_options import TestOptions\n",
    "from models import create_model\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "# ── 1) Parse TestOptions via sys.argv ─────────────────────────────────────────\n",
    "sys.argv = [\n",
    "    'test.py',\n",
    "    '--dataroot',   './my_dataset',\n",
    "    '--name',       'human2arcane',\n",
    "    '--model',      'cycle_gan',\n",
    "    '--phase',      'test',\n",
    "    '--num_test',   '1',\n",
    "    '--load_size',  '256',\n",
    "    '--crop_size',  '256',\n",
    "]\n",
    "opt = TestOptions().parse()\n",
    "model = create_model(opt)\n",
    "model.setup(opt)\n",
    "model.eval()\n",
    "\n",
    "# ── 2) Image preprocessing transform ─────────────────────────────────────────\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# ── 3) Face detector ─────────────────────────────────────────────────────────\n",
    "mtcnn = MTCNN(keep_all=False)\n",
    "\n",
    "# ── 4) Start webcam capture ──────────────────────────────────────────────────\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"Press 'q' to quit.\")\n",
    "\n",
    "# Define percentage margin around detected face\n",
    "MARGIN_PCT = 0.3  # 30% expansion in all directions\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    boxes, _ = mtcnn.detect(img_pil)\n",
    "    output = frame.copy()\n",
    "\n",
    "    if boxes is not None and len(boxes):\n",
    "        h, w = frame.shape[:2]\n",
    "        x1, y1, x2, y2 = boxes[0]\n",
    "\n",
    "        # Expand crop with margin\n",
    "        box_w, box_h = x2 - x1, y2 - y1\n",
    "        margin_w = box_w * MARGIN_PCT\n",
    "        margin_h = box_h * MARGIN_PCT\n",
    "\n",
    "        # Apply margin and clip to image boundaries\n",
    "        x1 = int(max(0, x1 - margin_w))\n",
    "        y1 = int(max(0, y1 - margin_h))\n",
    "        x2 = int(min(w, x2 + margin_w))\n",
    "        y2 = int(min(h, y2 + margin_h))\n",
    "\n",
    "        # Crop, preprocess and stylize\n",
    "        face_crop = img_pil.crop((x1, y1, x2, y2))\n",
    "        face_tensor = preprocess(face_crop).unsqueeze(0).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            fake = model.netG_A(face_tensor)\n",
    "\n",
    "        # Convert to uint8 RGB image\n",
    "        fake_np = fake.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "        fake_img = ((fake_np + 1) / 2 * 255).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "        # Resize to match original crop and convert to BGR for OpenCV\n",
    "        w_box, h_box = x2 - x1, y2 - y1\n",
    "        fake_resized = cv2.resize(fake_img, (w_box, h_box), interpolation=cv2.INTER_LINEAR)\n",
    "        fake_resized = cv2.cvtColor(fake_resized, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Paste back into frame\n",
    "        output[y1:y2, x1:x2] = fake_resized\n",
    "\n",
    "    # Show result\n",
    "    cv2.imshow('Human 2 Arcane Live', output)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9effaa-9765-4369-b3f9-8402492e740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from options.test_options import TestOptions\n",
    "from models import create_model\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "# ── 1) Parse TestOptions via sys.argv ─────────────────────────────────────────\n",
    "sys.argv = [\n",
    "    'test.py',\n",
    "    '--dataroot', './my_dataset',\n",
    "    '--name', 'human2arcane',\n",
    "    '--model', 'cycle_gan',\n",
    "    '--phase', 'test',\n",
    "    '--num_test', '1',\n",
    "    '--load_size', '256',\n",
    "    '--crop_size', '256',\n",
    "]\n",
    "opt = TestOptions().parse()\n",
    "model = create_model(opt)\n",
    "model.setup(opt)\n",
    "model.eval()\n",
    "\n",
    "# ── 2) Preprocessing ─────────────────────────────────────────────────────────\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# ── 3) Face detector ─────────────────────────────────────────────────────────\n",
    "mtcnn = MTCNN(keep_all=False)\n",
    "\n",
    "# ── 4) Video stream ──────────────────────────────────────────────────────────\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_pil = Image.fromarray(img_rgb)\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    # ── Stylize entire frame ────────────────────────────────────────────────\n",
    "    full_tensor = preprocess(img_pil).unsqueeze(0).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        stylized_full = model.netG_A(full_tensor)\n",
    "    stylized_np = stylized_full.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "    stylized_bgr = ((stylized_np + 1) / 2 * 255).clip(0, 255).astype(np.uint8)\n",
    "    stylized_bgr = cv2.cvtColor(stylized_bgr, cv2.COLOR_RGB2BGR)\n",
    "    stylized_bgr = cv2.resize(stylized_bgr, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # ── Face detection ─────────────────────────────────────────────────────\n",
    "    boxes, _ = mtcnn.detect(img_pil)\n",
    "    output = stylized_bgr.copy()\n",
    "\n",
    "    if boxes is not None and len(boxes):\n",
    "        x1, y1, x2, y2 = [int(v) for v in boxes[0]]\n",
    "        \n",
    "        # Apply padding around face (percent-based)\n",
    "        pad_pct = 0.4  # increase for wider context\n",
    "        box_w, box_h = x2 - x1, y2 - y1\n",
    "        pad_w, pad_h = int(pad_pct * box_w), int(pad_pct * box_h)\n",
    "\n",
    "        x1_pad = max(0, x1 - pad_w)\n",
    "        y1_pad = max(0, y1 - pad_h)\n",
    "        x2_pad = min(w, x2 + pad_w)\n",
    "        y2_pad = min(h, y2 + pad_h)\n",
    "\n",
    "        # Crop and preprocess face region\n",
    "        face_crop = img_pil.crop((x1_pad, y1_pad, x2_pad, y2_pad))\n",
    "        face_tensor = preprocess(face_crop).unsqueeze(0).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            stylized_face = model.netG_A(face_tensor)\n",
    "\n",
    "        face_np = stylized_face.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "        face_img = ((face_np + 1) / 2 * 255).clip(0, 255).astype(np.uint8)\n",
    "        face_bgr = cv2.cvtColor(face_img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Resize stylized face to original padded region size\n",
    "        face_bgr_resized = cv2.resize(face_bgr, (x2_pad - x1_pad, y2_pad - y1_pad), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        # Soft circular mask for blending\n",
    "        mask = np.zeros((y2_pad - y1_pad, x2_pad - x1_pad, 3), dtype=np.float32)\n",
    "        cv2.circle(mask, ((x2_pad - x1_pad)//2, (y2_pad - y1_pad)//2), min((x2_pad - x1_pad), (y2_pad - y1_pad))//2, (1,1,1), -1)\n",
    "        mask = cv2.GaussianBlur(mask, (31, 31), 0)\n",
    "\n",
    "        # Extract region of interest from background\n",
    "        roi = output[y1_pad:y2_pad, x1_pad:x2_pad]\n",
    "        if roi.shape == face_bgr_resized.shape:\n",
    "            blended = (mask * face_bgr_resized + (1 - mask) * roi).astype(np.uint8)\n",
    "            output[y1_pad:y2_pad, x1_pad:x2_pad] = blended\n",
    "\n",
    "    # ── Display ─────────────────────────────────────────────────────────────\n",
    "    cv2.imshow('Human 2 Arcane Composite', output)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37525ada-23bd-492d-86a2-3972f712ab5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from options.test_options import TestOptions\n",
    "from models import create_model\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "# ── 1) Parse TestOptions via sys.argv ─────────────────────────────────────────\n",
    "sys.argv = [\n",
    "    'test.py',\n",
    "    '--dataroot', './my_dataset',\n",
    "    '--name', 'human2arcane',\n",
    "    '--model', 'cycle_gan',\n",
    "    '--phase', 'test',\n",
    "    '--num_test', '1',\n",
    "    '--load_size', '256',\n",
    "    '--crop_size', '256',\n",
    "]\n",
    "opt = TestOptions().parse()\n",
    "model = create_model(opt)\n",
    "model.setup(opt)\n",
    "model.eval()\n",
    "\n",
    "# ── 2) Preprocessing ─────────────────────────────────────────────────────────\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# ── 3) Face detector ─────────────────────────────────────────────────────────\n",
    "mtcnn = MTCNN(keep_all=False)\n",
    "\n",
    "# ── 4) Video stream ──────────────────────────────────────────────────────────\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_pil = Image.fromarray(img_rgb)\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    # ── Stylize entire frame ────────────────────────────────────────────────\n",
    "    full_tensor = preprocess(img_pil).unsqueeze(0).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        stylized_full = model.netG_A(full_tensor)\n",
    "    stylized_np = stylized_full.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "    stylized_bgr = ((stylized_np + 1) / 2 * 255).clip(0, 255).astype(np.uint8)\n",
    "    stylized_bgr = cv2.cvtColor(stylized_bgr, cv2.COLOR_RGB2BGR)\n",
    "    stylized_bgr = cv2.resize(stylized_bgr, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # ── Face detection ─────────────────────────────────────────────────────\n",
    "    boxes, _ = mtcnn.detect(img_pil)\n",
    "    output = stylized_bgr.copy()\n",
    "\n",
    "    if boxes is not None and len(boxes):\n",
    "        x1, y1, x2, y2 = [int(v) for v in boxes[0]]\n",
    "\n",
    "        # Padding as percentage\n",
    "        pad_pct = 0.4\n",
    "        box_w, box_h = x2 - x1, y2 - y1\n",
    "        pad_w, pad_h = int(pad_pct * box_w), int(pad_pct * box_h)\n",
    "\n",
    "        x1_pad = max(0, x1 - pad_w)\n",
    "        y1_pad = max(0, y1 - pad_h)\n",
    "        x2_pad = min(w, x2 + pad_w)\n",
    "        y2_pad = min(h, y2 + pad_h)\n",
    "\n",
    "        # Crop and stylize face\n",
    "        face_crop = img_pil.crop((x1_pad, y1_pad, x2_pad, y2_pad))\n",
    "        face_tensor = preprocess(face_crop).unsqueeze(0).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            stylized_face = model.netG_A(face_tensor)\n",
    "\n",
    "        face_np = stylized_face.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "        face_img = ((face_np + 1) / 2 * 255).clip(0, 255).astype(np.uint8)\n",
    "        face_bgr = cv2.cvtColor(face_img, cv2.COLOR_RGB2BGR)\n",
    "        face_bgr_resized = cv2.resize(face_bgr, (x2_pad - x1_pad, y2_pad - y1_pad), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        # Create a smooth radial blend mask\n",
    "        h_mask, w_mask = y2_pad - y1_pad, x2_pad - x1_pad\n",
    "        Y, X = np.ogrid[:h_mask, :w_mask]\n",
    "        center_y, center_x = h_mask / 2, w_mask / 2\n",
    "        dist_from_center = np.sqrt((X - center_x)**2 + (Y - center_y)**2)\n",
    "        max_radius = np.sqrt(center_x**2 + center_y**2)\n",
    "        radial_mask = 1 - (dist_from_center / max_radius)\n",
    "        radial_mask = np.clip(radial_mask, 0, 1)\n",
    "        radial_mask = cv2.GaussianBlur(radial_mask, (81, 81), 0)\n",
    "        mask = np.dstack([radial_mask] * 3).astype(np.float32)\n",
    "\n",
    "        # Blend face into stylized background\n",
    "        roi = output[y1_pad:y2_pad, x1_pad:x2_pad]\n",
    "        if roi.shape == face_bgr_resized.shape:\n",
    "            blended = (mask * face_bgr_resized + (1 - mask) * roi).astype(np.uint8)\n",
    "            output[y1_pad:y2_pad, x1_pad:x2_pad] = blended\n",
    "\n",
    "    # ── Display ─────────────────────────────────────────────────────────────\n",
    "    cv2.imshow('Human 2 Arcane Composite', output)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3df7924-3711-46a1-8f0c-f7b30207bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from options.test_options import TestOptions\n",
    "from models import create_model\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "# ── 1) Parse TestOptions via sys.argv ─────────────────────────────────────────\n",
    "sys.argv = [\n",
    "    'test.py',\n",
    "    '--dataroot', './my_dataset',\n",
    "    '--name', 'human2arcane',\n",
    "    '--model', 'cycle_gan',\n",
    "    '--phase', 'test',\n",
    "    '--num_test', '1',\n",
    "    '--load_size', '256',\n",
    "    '--crop_size', '256',\n",
    "]\n",
    "opt = TestOptions().parse()\n",
    "model = create_model(opt)\n",
    "model.setup(opt)\n",
    "model.eval()\n",
    "\n",
    "# ── 2) Preprocessing (no Resize here!) ────────────────────────────────────────\n",
    "to_tensor = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "resize_256 = transforms.Resize((256, 256))\n",
    "\n",
    "# ── 3) Face detector ─────────────────────────────────────────────────────────\n",
    "mtcnn = MTCNN(keep_all=False)\n",
    "\n",
    "# ── 4) Video stream ──────────────────────────────────────────────────────────\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_pil = Image.fromarray(img_rgb)\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    # ── Stylize full frame ──────────────────────────────────────────────────\n",
    "    full_img_resized = resize_256(img_pil)\n",
    "    full_tensor = to_tensor(full_img_resized).unsqueeze(0).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        stylized_full = model.netG_A(full_tensor)\n",
    "\n",
    "    stylized_np = stylized_full.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "    stylized_bgr = ((stylized_np + 1) / 2 * 255).clip(0, 255).astype(np.uint8)\n",
    "    stylized_bgr = cv2.cvtColor(stylized_bgr, cv2.COLOR_RGB2BGR)\n",
    "    stylized_bgr = cv2.resize(stylized_bgr, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # ── Face detection ─────────────────────────────────────────────────────\n",
    "    boxes, _ = mtcnn.detect(img_pil)\n",
    "    output = stylized_bgr.copy()\n",
    "\n",
    "    if boxes is not None and len(boxes):\n",
    "        x1, y1, x2, y2 = [int(v) for v in boxes[0]]\n",
    "\n",
    "        # Padding as percentage\n",
    "        pad_pct = 0.4\n",
    "        box_w, box_h = x2 - x1, y2 - y1\n",
    "        pad_w, pad_h = int(pad_pct * box_w), int(pad_pct * box_h)\n",
    "\n",
    "        x1_pad = max(0, x1 - pad_w)\n",
    "        y1_pad = max(0, y1 - pad_h)\n",
    "        x2_pad = min(w, x2 + pad_w)\n",
    "        y2_pad = min(h, y2 + pad_h)\n",
    "\n",
    "        # ── Crop original resolution, then resize ──────────────────────────\n",
    "        face_crop = img_pil.crop((x1_pad, y1_pad, x2_pad, y2_pad))\n",
    "        face_resized = resize_256(face_crop)\n",
    "        face_tensor = to_tensor(face_resized).unsqueeze(0).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            stylized_face = model.netG_A(face_tensor)\n",
    "\n",
    "        face_np = stylized_face.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "        face_img = ((face_np + 1) / 2 * 255).clip(0, 255).astype(np.uint8)\n",
    "        face_bgr = cv2.cvtColor(face_img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Resize stylized face back to match the original padded region\n",
    "        target_w = x2_pad - x1_pad\n",
    "        target_h = y2_pad - y1_pad\n",
    "        face_bgr_resized = cv2.resize(face_bgr, (target_w, target_h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        # ── Create radial blend mask ──────────────────────────────────────\n",
    "        Y, X = np.ogrid[:target_h, :target_w]\n",
    "        center_y, center_x = target_h / 2, target_w / 2\n",
    "        dist_from_center = np.sqrt((X - center_x) ** 2 + (Y - center_y) ** 2)\n",
    "        max_radius = np.sqrt(center_x**2 + center_y**2)\n",
    "        radial_mask = 1 - (dist_from_center / max_radius)\n",
    "        radial_mask = np.clip(radial_mask, 0, 1)\n",
    "        radial_mask = cv2.GaussianBlur(radial_mask, (81, 81), 0)\n",
    "        mask = np.dstack([radial_mask] * 3).astype(np.float32)\n",
    "\n",
    "        # ── Blend into stylized background ────────────────────────────────\n",
    "        roi = output[y1_pad:y2_pad, x1_pad:x2_pad]\n",
    "        if roi.shape == face_bgr_resized.shape:\n",
    "            blended = (mask * face_bgr_resized + (1 - mask) * roi).astype(np.uint8)\n",
    "            output[y1_pad:y2_pad, x1_pad:x2_pad] = blended\n",
    "\n",
    "    # ── Show final composite ───────────────────────────────────────────────\n",
    "    cv2.imshow('Human 2 Arcane Composite', output)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25c741e-2077-434c-ba84-c171dc5be654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from options.test_options import TestOptions\n",
    "from models import create_model\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "# ── 1) Parse TestOptions via sys.argv ─────────────────────────────────────────\n",
    "sys.argv = [\n",
    "    'test.py',\n",
    "    '--dataroot', './my_dataset',\n",
    "    '--name', 'human2arcane',\n",
    "    '--model', 'cycle_gan',\n",
    "    '--phase', 'test',\n",
    "    '--num_test', '1',\n",
    "    '--load_size', '256',\n",
    "    '--crop_size', '256',\n",
    "]\n",
    "opt = TestOptions().parse()\n",
    "model = create_model(opt)\n",
    "model.setup(opt)\n",
    "model.eval()\n",
    "\n",
    "# ── 2) Preprocessing ─────────────────────────────────────────────────────────\n",
    "to_tensor = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "resize_256 = transforms.Resize((256, 256))\n",
    "\n",
    "# ── 3) Face detector ─────────────────────────────────────────────────────────\n",
    "mtcnn = MTCNN(keep_all=False)\n",
    "\n",
    "# ── 4) Video stream ──────────────────────────────────────────────────────────\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_pil = Image.fromarray(img_rgb)\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    # ── Stylize full frame ──────────────────────────────────────────────────\n",
    "    full_img_resized = resize_256(img_pil)\n",
    "    full_tensor = to_tensor(full_img_resized).unsqueeze(0).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        stylized_full = model.netG_A(full_tensor)\n",
    "\n",
    "    stylized_np = stylized_full.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "    stylized_bgr = ((stylized_np + 1) / 2 * 255).clip(0, 255).astype(np.uint8)\n",
    "    stylized_bgr = cv2.cvtColor(stylized_bgr, cv2.COLOR_RGB2BGR)\n",
    "    stylized_bgr = cv2.resize(stylized_bgr, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # ── Face detection ─────────────────────────────────────────────────────\n",
    "    boxes, _ = mtcnn.detect(img_pil)\n",
    "    output = stylized_bgr.copy()\n",
    "\n",
    "    if boxes is not None and len(boxes):\n",
    "        x1, y1, x2, y2 = [int(v) for v in boxes[0]]\n",
    "\n",
    "        # Padding as percentage\n",
    "        pad_pct = 0.4\n",
    "        box_w, box_h = x2 - x1, y2 - y1\n",
    "        pad_w, pad_h = int(pad_pct * box_w), int(pad_pct * box_h)\n",
    "\n",
    "        x1_pad = max(0, x1 - pad_w)\n",
    "        y1_pad = max(0, y1 - pad_h)\n",
    "        x2_pad = min(w, x2 + pad_w)\n",
    "        y2_pad = min(h, y2 + pad_h)\n",
    "\n",
    "        # ── Crop & resize ──────────────────────────────────────────────────\n",
    "        face_crop = img_pil.crop((x1_pad, y1_pad, x2_pad, y2_pad))\n",
    "        face_resized = resize_256(face_crop)\n",
    "        face_tensor = to_tensor(face_resized).unsqueeze(0).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            stylized_face = model.netG_A(face_tensor)\n",
    "\n",
    "        face_np = stylized_face.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "        face_img = ((face_np + 1) / 2 * 255).clip(0, 255).astype(np.uint8)\n",
    "        face_bgr = cv2.cvtColor(face_img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Resize stylized face back to ROI\n",
    "        target_w = x2_pad - x1_pad\n",
    "        target_h = y2_pad - y1_pad\n",
    "        face_bgr_resized = cv2.resize(face_bgr, (target_w, target_h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        # ── Create smoother radial gradient mask using sigmoid ─────────────\n",
    "        Y, X = np.ogrid[:target_h, :target_w]\n",
    "        center_y, center_x = target_h / 2, target_w / 2\n",
    "        dist = np.sqrt((X - center_x)**2 + (Y - center_y)**2)\n",
    "        max_dist = np.sqrt(center_x**2 + center_y**2)\n",
    "        norm_dist = dist / max_dist\n",
    "\n",
    "        # Sigmoid-based smooth mask\n",
    "        sharpness = 10  # increase for sharper center\n",
    "        mask = 1 / (1 + np.exp(sharpness * (norm_dist - 0.5)))\n",
    "        mask = cv2.GaussianBlur(mask, (101, 101), 0)  # soften further\n",
    "        mask = np.dstack([mask] * 3).astype(np.float32)\n",
    "\n",
    "        # ── Blend face into background ─────────────────────────────────────\n",
    "        roi = output[y1_pad:y2_pad, x1_pad:x2_pad]\n",
    "        if roi.shape == face_bgr_resized.shape:\n",
    "            blended = (mask * face_bgr_resized + (1 - mask) * roi).astype(np.uint8)\n",
    "            output[y1_pad:y2_pad, x1_pad:x2_pad] = blended\n",
    "\n",
    "    cv2.imshow('Human 2 Arcane - Smooth Transition', output)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "542780df-0437-4327-aa50-12250344c0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "             aspect_ratio: 1.0                           \n",
      "               batch_size: 1                             \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "                crop_size: 256                           \n",
      "                 dataroot: ./my_dataset                  \t[default: None]\n",
      "             dataset_mode: unaligned                     \n",
      "                direction: AtoB                          \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "                     eval: False                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: False                         \t[default: None]\n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 256                           \n",
      "         max_dataset_size: inf                           \n",
      "                    model: cycle_gan                     \t[default: test]\n",
      "               n_layers_D: 3                             \n",
      "                     name: human2arcane                  \t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "               no_dropout: True                          \n",
      "                  no_flip: False                         \n",
      "                     norm: instance                      \n",
      "                 num_test: 1                             \t[default: 50]\n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: test                          \n",
      "               preprocess: resize_and_crop               \n",
      "              results_dir: ./results/                    \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "                use_wandb: False                         \n",
      "                  verbose: False                         \n",
      "       wandb_project_name: CycleGAN-and-pix2pix          \n",
      "----------------- End -------------------\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [CycleGANModel] was created\n",
      "loading the model from ./checkpoints\\human2arcane\\latest_net_G_A.pth\n",
      "loading the model from ./checkpoints\\human2arcane\\latest_net_G_B.pth\n",
      "---------- Networks initialized -------------\n",
      "[Network G_A] Total number of parameters : 11.378 M\n",
      "[Network G_B] Total number of parameters : 11.378 M\n",
      "-----------------------------------------------\n",
      "Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from options.test_options import TestOptions\n",
    "from models import create_model\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "# ── 1) Parse TestOptions ─────────────────────────────────────────────────────\n",
    "sys.argv = [\n",
    "    'test.py',\n",
    "    '--dataroot',   './my_dataset',\n",
    "    '--name',       'human2arcane',\n",
    "    '--model',      'cycle_gan',\n",
    "    '--phase',      'test',\n",
    "    '--num_test',   '1',\n",
    "    '--load_size',  '256',\n",
    "    '--crop_size',  '256',\n",
    "]\n",
    "opt = TestOptions().parse()\n",
    "model = create_model(opt)\n",
    "model.setup(opt)\n",
    "model.eval()\n",
    "\n",
    "# ── 2) Preprocessing transform ───────────────────────────────────────────────\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# ── 3) Face detector ─────────────────────────────────────────────────────────\n",
    "mtcnn = MTCNN(keep_all=False)\n",
    "\n",
    "# ── 4) Start webcam ──────────────────────────────────────────────────────────\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    boxes, _ = mtcnn.detect(img_pil)\n",
    "    output = frame.copy()\n",
    "\n",
    "    if boxes is not None and len(boxes):\n",
    "        h, w = frame.shape[:2]\n",
    "        box = boxes[0]\n",
    "\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        face_w, face_h = x2 - x1, y2 - y1\n",
    "\n",
    "        # Increase crop region by percentage\n",
    "        expand_pct = 0.6  # 60% padding\n",
    "        pad_w = int(face_w * expand_pct / 2)\n",
    "        pad_h = int(face_h * expand_pct / 2)\n",
    "\n",
    "        x1_pad = max(0, x1 - pad_w)\n",
    "        y1_pad = max(0, y1 - pad_h)\n",
    "        x2_pad = min(w, x2 + pad_w)\n",
    "        y2_pad = min(h, y2 + pad_h)\n",
    "\n",
    "        # --- Stylize full frame ---\n",
    "        full_img = img_pil.resize((256, 256))\n",
    "        full_tensor = preprocess(full_img).unsqueeze(0).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            full_fake = model.netG_A(full_tensor)\n",
    "        full_np = full_fake.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "        full_img_np = ((full_np + 1) / 2 * 255).clip(0, 255).astype(np.uint8)\n",
    "        output = cv2.cvtColor(full_img_np, cv2.COLOR_RGB2BGR)\n",
    "        output = cv2.resize(output, (w, h))\n",
    "\n",
    "        # --- Stylize cropped face at high resolution ---\n",
    "        face_crop = img_pil.crop((x1_pad, y1_pad, x2_pad, y2_pad))\n",
    "        face_tensor = preprocess(face_crop).unsqueeze(0).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            face_fake = model.netG_A(face_tensor)\n",
    "\n",
    "        face_np = face_fake.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "        face_img = ((face_np + 1) / 2 * 255).clip(0, 255).astype(np.uint8)\n",
    "        face_bgr = cv2.cvtColor(face_img, cv2.COLOR_RGB2BGR)\n",
    "        target_w, target_h = x2_pad - x1_pad, y2_pad - y1_pad\n",
    "        face_bgr_resized = cv2.resize(face_bgr, (target_w, target_h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        # --- Gaussian blending mask ---\n",
    "        mask = np.zeros((target_h, target_w), dtype=np.float32)\n",
    "        cv2.circle(mask, (target_w // 2, target_h // 2), min(target_w, target_h) // 2, 1.0, -1)\n",
    "        mask = cv2.GaussianBlur(mask, (61, 61), 0)\n",
    "        mask = np.expand_dims(mask, axis=2)\n",
    "        mask = np.repeat(mask, 3, axis=2)\n",
    "\n",
    "        roi = output[y1_pad:y2_pad, x1_pad:x2_pad]\n",
    "\n",
    "        # --- Match brightness/color using LAB histogram alignment ---\n",
    "        if roi.shape == face_bgr_resized.shape:\n",
    "            face_lab = cv2.cvtColor(face_bgr_resized, cv2.COLOR_BGR2LAB).astype(np.float32)\n",
    "            roi_lab = cv2.cvtColor(roi, cv2.COLOR_BGR2LAB).astype(np.float32)\n",
    "\n",
    "            for c in range(3):\n",
    "                roi_mean, roi_std = roi_lab[:, :, c].mean(), roi_lab[:, :, c].std()\n",
    "                face_mean, face_std = face_lab[:, :, c].mean(), face_lab[:, :, c].std()\n",
    "                face_std = face_std if face_std > 1e-6 else 1.0\n",
    "                face_lab[:, :, c] = (face_lab[:, :, c] - face_mean) / face_std * roi_std + roi_mean\n",
    "                face_lab[:, :, c] = np.clip(face_lab[:, :, c], 0, 255)\n",
    "\n",
    "            face_bgr_resized = cv2.cvtColor(face_lab.astype(np.uint8), cv2.COLOR_LAB2BGR)\n",
    "\n",
    "        # --- Blend face and background ---\n",
    "        blended = (mask * face_bgr_resized + (1 - mask) * roi).astype(np.uint8)\n",
    "        output[y1_pad:y2_pad, x1_pad:x2_pad] = blended\n",
    "\n",
    "    cv2.imshow('Human 2 Arcane Composite', output)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d59e174-dae8-48b7-a0c6-0bc0ef3fd7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#last version but for webcam\n",
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from options.test_options import TestOptions\n",
    "from models import create_model\n",
    "from facenet_pytorch import MTCNN\n",
    "import pyvirtualcam\n",
    "\n",
    "# ── 1) Parse TestOptions ─────────────────────────────────────────────────────\n",
    "sys.argv = [\n",
    "    'test.py',\n",
    "    '--dataroot',   './my_dataset',\n",
    "    '--name',       'human2arcane',\n",
    "    '--model',      'cycle_gan',\n",
    "    '--phase',      'test',\n",
    "    '--num_test',   '1',\n",
    "    '--load_size',  '256',\n",
    "    '--crop_size',  '256',\n",
    "]\n",
    "opt = TestOptions().parse()\n",
    "model = create_model(opt)\n",
    "model.setup(opt)\n",
    "model.eval()\n",
    "\n",
    "# ── 2) Preprocessing transform ────────────────────────────────────────────\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# ── 3) Face detector ────────────────────────────────────────────\n",
    "mtcnn = MTCNN(keep_all=False)\n",
    "\n",
    "# ── 4) Start webcam ───────────────────────────────────────────\n",
    "cap = cv2.VideoCapture(0)\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    raise RuntimeError(\"Failed to read from webcam.\")\n",
    "h, w = frame.shape[:2]\n",
    "\n",
    "with pyvirtualcam.Camera(width=w, height=h, fps=30, print_fps=True) as cam:\n",
    "    print(f'Virtual camera started: {cam.device}')\n",
    "    print(\"Press Ctrl+C to quit.\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            boxes, _ = mtcnn.detect(img_pil)\n",
    "            output = frame.copy()\n",
    "\n",
    "            if boxes is not None and len(boxes):\n",
    "                box = boxes[0]\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                face_w, face_h = x2 - x1, y2 - y1\n",
    "\n",
    "                expand_pct = 0.6\n",
    "                pad_w = int(face_w * expand_pct / 2)\n",
    "                pad_h = int(face_h * expand_pct / 2)\n",
    "\n",
    "                x1_pad = max(0, x1 - pad_w)\n",
    "                y1_pad = max(0, y1 - pad_h)\n",
    "                x2_pad = min(w, x2 + pad_w)\n",
    "                y2_pad = min(h, y2 + pad_h)\n",
    "\n",
    "                full_img = img_pil.resize((256, 256))\n",
    "                full_tensor = preprocess(full_img).unsqueeze(0).to(model.device)\n",
    "                with torch.no_grad():\n",
    "                    full_fake = model.netG_A(full_tensor)\n",
    "                full_np = full_fake.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "                full_img_np = ((full_np + 1) / 2 * 255).clip(0, 255).astype(np.uint8)\n",
    "                output = cv2.cvtColor(full_img_np, cv2.COLOR_RGB2BGR)\n",
    "                output = cv2.resize(output, (w, h))\n",
    "\n",
    "                face_crop = img_pil.crop((x1_pad, y1_pad, x2_pad, y2_pad))\n",
    "                face_tensor = preprocess(face_crop).unsqueeze(0).to(model.device)\n",
    "                with torch.no_grad():\n",
    "                    face_fake = model.netG_A(face_tensor)\n",
    "                face_np = face_fake.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "                face_img = ((face_np + 1) / 2 * 255).clip(0, 255).astype(np.uint8)\n",
    "                face_bgr = cv2.cvtColor(face_img, cv2.COLOR_RGB2BGR)\n",
    "                target_w, target_h = x2_pad - x1_pad, y2_pad - y1_pad\n",
    "                face_bgr_resized = cv2.resize(face_bgr, (target_w, target_h))\n",
    "\n",
    "                mask = np.zeros((target_h, target_w), dtype=np.float32)\n",
    "                cv2.circle(mask, (target_w // 2, target_h // 2), min(target_w, target_h) // 2, 1.0, -1)\n",
    "                mask = cv2.GaussianBlur(mask, (61, 61), 0)\n",
    "                mask = np.repeat(mask[:, :, np.newaxis], 3, axis=2)\n",
    "\n",
    "                roi = output[y1_pad:y2_pad, x1_pad:x2_pad]\n",
    "\n",
    "                if roi.shape == face_bgr_resized.shape:\n",
    "                    face_lab = cv2.cvtColor(face_bgr_resized, cv2.COLOR_BGR2LAB).astype(np.float32)\n",
    "                    roi_lab = cv2.cvtColor(roi, cv2.COLOR_BGR2LAB).astype(np.float32)\n",
    "\n",
    "                    for c in range(3):\n",
    "                        roi_mean, roi_std = roi_lab[:, :, c].mean(), roi_lab[:, :, c].std()\n",
    "                        face_mean, face_std = face_lab[:, :, c].mean(), face_lab[:, :, c].std()\n",
    "                        face_std = face_std if face_std > 1e-6 else 1.0\n",
    "                        face_lab[:, :, c] = (face_lab[:, :, c] - face_mean) / face_std * roi_std + roi_mean\n",
    "                        face_lab[:, :, c] = np.clip(face_lab[:, :, c], 0, 255)\n",
    "\n",
    "                    face_bgr_resized = cv2.cvtColor(face_lab.astype(np.uint8), cv2.COLOR_LAB2BGR)\n",
    "\n",
    "                blended = (mask * face_bgr_resized + (1 - mask) * roi).astype(np.uint8)\n",
    "                output[y1_pad:y2_pad, x1_pad:x2_pad] = blended\n",
    "\n",
    "            # Convert BGR to RGB for virtual cam\n",
    "            output_rgb = cv2.cvtColor(output, cv2.COLOR_BGR2RGB)\n",
    "            cam.send(output_rgb)\n",
    "            cam.sleep_until_next_frame()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Exiting...\")\n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e5f5db-4b36-408a-8d34-c726e3e27444",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for zoom with brightness slider\n",
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "import torchvision.transforms as transforms\n",
    "from options.test_options import TestOptions\n",
    "from models import create_model\n",
    "from facenet_pytorch import MTCNN\n",
    "import pyvirtualcam\n",
    "\n",
    "# ── 1) Parse TestOptions ─────────────────────────────────────────────────────\n",
    "sys.argv = [\n",
    "    'test.py',\n",
    "    '--dataroot', './my_dataset',\n",
    "    '--name', 'human2arcane',\n",
    "    '--model', 'cycle_gan',\n",
    "    '--phase', 'test',\n",
    "    '--num_test', '1',\n",
    "    '--load_size', '256',\n",
    "    '--crop_size', '256',\n",
    "]\n",
    "opt = TestOptions().parse()\n",
    "model = create_model(opt)\n",
    "model.setup(opt)\n",
    "model.eval()\n",
    "\n",
    "# ── 2) Preprocessing transform ───────────────────────────────────────────────\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# ── 3) Face detector ─────────────────────────────────────────────────────────\n",
    "mtcnn = MTCNN(keep_all=False)\n",
    "\n",
    "# ── 4) Webcam init ────────────────────────────────────────────────────────────\n",
    "cap = cv2.VideoCapture(0)\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    raise RuntimeError(\"Failed to read from webcam.\")\n",
    "h, w = frame.shape[:2]\n",
    "\n",
    "# ── 5) Brightness trackbar ────────────────────────────────────────────────────\n",
    "def nothing(x): pass\n",
    "cv2.namedWindow(\"Brightness\")\n",
    "cv2.createTrackbar(\"Level\", \"Brightness\", 100, 200, nothing)  # 100 is neutral\n",
    "\n",
    "# ── 6) Start virtual camera ───────────────────────────────────────────────────\n",
    "with pyvirtualcam.Camera(width=w, height=h, fps=30) as cam:\n",
    "    print(f\"Virtual camera started: {cam.device}\")\n",
    "    print(\"Press 'q' to quit.\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Read trackbar value and apply brightness\n",
    "        brightness_level = cv2.getTrackbarPos(\"Level\", \"Brightness\")\n",
    "        brightness_factor = brightness_level / 100.0  # 1.0 = neutral\n",
    "        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img_pil = Image.fromarray(img_rgb)\n",
    "        img_pil = ImageEnhance.Brightness(img_pil).enhance(brightness_factor)\n",
    "\n",
    "        boxes, _ = mtcnn.detect(img_pil)\n",
    "        output = frame.copy()\n",
    "\n",
    "        if boxes is not None and len(boxes):\n",
    "            x1, y1, x2, y2 = map(int, boxes[0])\n",
    "            face_w, face_h = x2 - x1, y2 - y1\n",
    "\n",
    "            expand_pct = 0.6\n",
    "            pad_w = int(face_w * expand_pct / 2)\n",
    "            pad_h = int(face_h * expand_pct / 2)\n",
    "\n",
    "            x1_pad = max(0, x1 - pad_w)\n",
    "            y1_pad = max(0, y1 - pad_h)\n",
    "            x2_pad = min(w, x2 + pad_w)\n",
    "            y2_pad = min(h, y2 + pad_h)\n",
    "\n",
    "            # --- Stylize full frame ---\n",
    "            full_img = img_pil.resize((256, 256))\n",
    "            full_tensor = preprocess(full_img).unsqueeze(0).to(model.device)\n",
    "            with torch.no_grad():\n",
    "                full_fake = model.netG_A(full_tensor)\n",
    "            full_np = full_fake.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "            full_img_np = ((full_np + 1) / 2 * 255).clip(0, 255).astype(np.uint8)\n",
    "            output = cv2.cvtColor(full_img_np, cv2.COLOR_RGB2BGR)\n",
    "            output = cv2.resize(output, (w, h))\n",
    "\n",
    "            # --- Stylize cropped face at high resolution ---\n",
    "            face_crop = img_pil.crop((x1_pad, y1_pad, x2_pad, y2_pad))\n",
    "            face_tensor = preprocess(face_crop).unsqueeze(0).to(model.device)\n",
    "            with torch.no_grad():\n",
    "                face_fake = model.netG_A(face_tensor)\n",
    "            face_np = face_fake.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "            face_img = ((face_np + 1) / 2 * 255).clip(0, 255).astype(np.uint8)\n",
    "            face_bgr = cv2.cvtColor(face_img, cv2.COLOR_RGB2BGR)\n",
    "            target_w, target_h = x2_pad - x1_pad, y2_pad - y1_pad\n",
    "            face_bgr_resized = cv2.resize(face_bgr, (target_w, target_h))\n",
    "\n",
    "            # --- Gaussian mask ---\n",
    "            mask = np.zeros((target_h, target_w), dtype=np.float32)\n",
    "            cv2.circle(mask, (target_w // 2, target_h // 2), min(target_w, target_h) // 2, 1.0, -1)\n",
    "            mask = cv2.GaussianBlur(mask, (61, 61), 0)\n",
    "            mask = np.expand_dims(mask, axis=2)\n",
    "            mask = np.repeat(mask, 3, axis=2)\n",
    "\n",
    "            roi = output[y1_pad:y2_pad, x1_pad:x2_pad]\n",
    "            if roi.shape == face_bgr_resized.shape:\n",
    "                face_lab = cv2.cvtColor(face_bgr_resized, cv2.COLOR_BGR2LAB).astype(np.float32)\n",
    "                roi_lab = cv2.cvtColor(roi, cv2.COLOR_BGR2LAB).astype(np.float32)\n",
    "\n",
    "                for c in range(3):\n",
    "                    roi_mean, roi_std = roi_lab[:, :, c].mean(), roi_lab[:, :, c].std()\n",
    "                    face_mean, face_std = face_lab[:, :, c].mean(), face_lab[:, :, c].std()\n",
    "                    face_std = face_std if face_std > 1e-6 else 1.0\n",
    "                    face_lab[:, :, c] = (face_lab[:, :, c] - face_mean) / face_std * roi_std + roi_mean\n",
    "                    face_lab[:, :, c] = np.clip(face_lab[:, :, c], 0, 255)\n",
    "\n",
    "                face_bgr_resized = cv2.cvtColor(face_lab.astype(np.uint8), cv2.COLOR_LAB2BGR)\n",
    "\n",
    "            blended = (mask * face_bgr_resized + (1 - mask) * roi).astype(np.uint8)\n",
    "            output[y1_pad:y2_pad, x1_pad:x2_pad] = blended\n",
    "\n",
    "        # Show locally\n",
    "        cv2.imshow(\"Human 2 Arcane Composite\", output)\n",
    "\n",
    "        # Stream to virtual camera\n",
    "        cam.send(cv2.cvtColor(output, cv2.COLOR_BGR2RGB))\n",
    "        cam.sleep_until_next_frame()\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1722bd1a-0780-433e-b8f6-0143d6cf4063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "             aspect_ratio: 1.0                           \n",
      "               batch_size: 1                             \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "                crop_size: 256                           \n",
      "                 dataroot: ./my_dataset                  \t[default: None]\n",
      "             dataset_mode: unaligned                     \n",
      "                direction: AtoB                          \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "                     eval: False                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: False                         \t[default: None]\n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 256                           \n",
      "         max_dataset_size: inf                           \n",
      "                    model: cycle_gan                     \t[default: test]\n",
      "               n_layers_D: 3                             \n",
      "                     name: human2arcane                  \t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "               no_dropout: True                          \n",
      "                  no_flip: False                         \n",
      "                     norm: instance                      \n",
      "                 num_test: 1                             \t[default: 50]\n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: test                          \n",
      "               preprocess: resize_and_crop               \n",
      "              results_dir: ./results/                    \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "                use_wandb: False                         \n",
      "                  verbose: False                         \n",
      "       wandb_project_name: CycleGAN-and-pix2pix          \n",
      "----------------- End -------------------\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [CycleGANModel] was created\n",
      "loading the model from ./checkpoints\\human2arcane\\latest_net_G_A.pth\n",
      "loading the model from ./checkpoints\\human2arcane\\latest_net_G_B.pth\n",
      "---------- Networks initialized -------------\n",
      "[Network G_A] Total number of parameters : 11.378 M\n",
      "[Network G_B] Total number of parameters : 11.378 M\n",
      "-----------------------------------------------\n",
      "Virtual camera started: OBS Virtual Camera\n",
      "Press Ctrl+C to quit.\n",
      "3.7 fps | 100 %\n",
      "4.0 fps | 100 %\n",
      "4.2 fps | 100 %\n",
      "4.3 fps | 100 %\n",
      "4.2 fps | 100 %\n",
      "4.5 fps | 100 %\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from options.test_options import TestOptions\n",
    "from models import create_model\n",
    "from facenet_pytorch import MTCNN\n",
    "import pyvirtualcam\n",
    "\n",
    "# ── 1) Parse TestOptions ─────────────────────────────────────────────────────\n",
    "sys.argv = [\n",
    "    'test.py',\n",
    "    '--dataroot',    './my_dataset',\n",
    "    '--name',        'human2arcane',\n",
    "    '--model',       'cycle_gan',\n",
    "    '--phase',       'test',\n",
    "    '--num_test',    '1',\n",
    "    '--load_size',   '256',\n",
    "    '--crop_size',   '256',\n",
    "]\n",
    "opt = TestOptions().parse()\n",
    "model = create_model(opt)\n",
    "model.setup(opt)\n",
    "model.eval()\n",
    "\n",
    "# ── 2) Preprocessing transform ────────────────────────────────────────────\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# ── 3) Face detector ────────────────────────────────────────────\n",
    "mtcnn = MTCNN(keep_all=False)\n",
    "\n",
    "# ── 4) Initialize contrast, gamma, and brightness ───────────────────────────\n",
    "contrast = 1.0\n",
    "gamma = 1.0\n",
    "brightness = 0\n",
    "\n",
    "def update_contrast(new_contrast):\n",
    "    global contrast\n",
    "    contrast = new_contrast / 100.0\n",
    "\n",
    "def update_gamma(new_gamma):\n",
    "    global gamma\n",
    "    gamma = new_gamma / 100.0\n",
    "\n",
    "def update_brightness(new_brightness):\n",
    "    global brightness\n",
    "    brightness = int(new_brightness) - 100\n",
    "\n",
    "# ── 5) Start webcam and create sliders ───────────────────────────────────────────\n",
    "cap = cv2.VideoCapture(0)\n",
    "ret, initial_frame = cap.read()\n",
    "if not ret:\n",
    "    raise RuntimeError(\"Failed to read from webcam.\")\n",
    "h, w = initial_frame.shape[:2]\n",
    "\n",
    "cv2.namedWindow('Camera Input Adjustments')\n",
    "cv2.createTrackbar('Contrast', 'Camera Input Adjustments', 100, 200, update_contrast)\n",
    "cv2.createTrackbar('Gamma', 'Camera Input Adjustments', 100, 200, update_gamma)\n",
    "cv2.createTrackbar('Brightness', 'Camera Input Adjustments', 100, 200, update_brightness)\n",
    "\n",
    "with pyvirtualcam.Camera(width=w, height=h, fps=30, print_fps=True) as cam:\n",
    "    print(f'Virtual camera started: {cam.device}')\n",
    "    print(\"Press Ctrl+C to quit.\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Apply contrast, gamma, and brightness adjustments to the input frame\n",
    "            adjusted_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV).astype(np.float32)\n",
    "            h_channel, s_channel, v_channel = cv2.split(adjusted_frame)\n",
    "\n",
    "            # Contrast adjustment on Value channel\n",
    "            v_channel = np.clip(v_channel * contrast, 0, 255)\n",
    "\n",
    "            # Gamma adjustment on Value channel\n",
    "            invGamma = 1.0 / gamma\n",
    "            v_channel = np.power(v_channel / 255.0, invGamma) * 255.0\n",
    "            v_channel = np.clip(v_channel + brightness, 0, 255)\n",
    "\n",
    "            adjusted_frame_hsv = cv2.merge([h_channel, s_channel, v_channel]).astype(np.uint8)\n",
    "            adjusted_frame_bgr = cv2.cvtColor(adjusted_frame_hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "            img_pil = Image.fromarray(cv2.cvtColor(adjusted_frame_bgr, cv2.COLOR_BGR2RGB))\n",
    "            boxes, _ = mtcnn.detect(img_pil)\n",
    "            output = adjusted_frame_bgr.copy()\n",
    "\n",
    "            if boxes is not None and len(boxes):\n",
    "                box = boxes[0]\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                face_w, face_h = x2 - x1, y2 - y1\n",
    "\n",
    "                expand_pct = 0.6\n",
    "                pad_w = int(face_w * expand_pct / 2)\n",
    "                pad_h = int(face_h * expand_pct / 2)\n",
    "\n",
    "                x1_pad = max(0, x1 - pad_w)\n",
    "                y1_pad = max(0, y1 - pad_h)\n",
    "                x2_pad = min(w, x2 + pad_w)\n",
    "                y2_pad = min(h, y2 + pad_h)\n",
    "\n",
    "                full_img = img_pil.resize((256, 256))\n",
    "                full_tensor = preprocess(full_img).unsqueeze(0).to(model.device)\n",
    "                with torch.no_grad():\n",
    "                    full_fake = model.netG_A(full_tensor)\n",
    "                full_np = full_fake.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "                full_img_np = ((full_np + 1) / 2 * 255).clip(0, 255).astype(np.uint8)\n",
    "                output = cv2.cvtColor(full_img_np, cv2.COLOR_RGB2BGR)\n",
    "                output = cv2.resize(output, (w, h))\n",
    "\n",
    "                face_crop = img_pil.crop((x1_pad, y1_pad, x2_pad, y2_pad))\n",
    "                face_tensor = preprocess(face_crop).unsqueeze(0).to(model.device)\n",
    "                with torch.no_grad():\n",
    "                    face_fake = model.netG_A(face_tensor)\n",
    "                face_np = face_fake.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "                face_img = ((face_np + 1) / 2 * 255).clip(0, 255).astype(np.uint8)\n",
    "                face_bgr = cv2.cvtColor(face_img, cv2.COLOR_RGB2BGR)\n",
    "                target_w, target_h = x2_pad - x1_pad, y2_pad - y1_pad\n",
    "                face_bgr_resized = cv2.resize(face_bgr, (target_w, target_h))\n",
    "\n",
    "                mask = np.zeros((target_h, target_w), dtype=np.float32)\n",
    "                cv2.circle(mask, (target_w // 2, target_h // 2), min(target_w, target_h) // 2, 1.0, -1)\n",
    "                mask = cv2.GaussianBlur(mask, (61, 61), 0)\n",
    "                mask = np.repeat(mask[:, :, np.newaxis], 3, axis=2)\n",
    "\n",
    "                roi = output[y1_pad:y2_pad, x1_pad:x2_pad]\n",
    "\n",
    "                if roi.shape == face_bgr_resized.shape:\n",
    "                    face_lab = cv2.cvtColor(face_bgr_resized, cv2.COLOR_BGR2LAB).astype(np.float32)\n",
    "                    roi_lab = cv2.cvtColor(roi, cv2.COLOR_BGR2LAB).astype(np.float32)\n",
    "\n",
    "                    for c in range(3):\n",
    "                        roi_mean, roi_std = roi_lab[:, :, c].mean(), roi_lab[:, :, c].std()\n",
    "                        face_mean, face_std = face_lab[:, :, c].mean(), face_lab[:, :, c].std()\n",
    "                        face_std = face_std if face_std > 1e-6 else 1.0\n",
    "                        face_lab[:, :, c] = (face_lab[:, :, c] - face_mean) / face_std * roi_std + roi_mean\n",
    "                        face_lab[:, :, c] = np.clip(face_lab[:, :, c], 0, 255)\n",
    "\n",
    "                    face_bgr_resized = cv2.cvtColor(face_lab.astype(np.uint8), cv2.COLOR_LAB2BGR)\n",
    "\n",
    "                    blended = (mask * face_bgr_resized + (1 - mask) * roi).astype(np.uint8)\n",
    "                    output[y1_pad:y2_pad, x1_pad:x2_pad] = blended\n",
    "\n",
    "            # Convert BGR to RGB for virtual cam\n",
    "            output_rgb = cv2.cvtColor(output, cv2.COLOR_BGR2RGB)\n",
    "            cam.send(output_rgb)\n",
    "            cam.sleep_until_next_frame()\n",
    "\n",
    "            cv2.imshow('Camera Input Adjustments', adjusted_frame_bgr)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Exiting...\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347a79cd-2db3-4f1f-8dc6-81d76f510e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference face light\n",
    "\n",
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from options.test_options import TestOptions\n",
    "from models import create_model\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "# ── 1) Parse TestOptions ─────────────────────────────────────────────────────\n",
    "sys.argv = [\n",
    "    'test.py',\n",
    "    '--dataroot',   './my_dataset',\n",
    "    '--name',       'human2arcane',\n",
    "    '--model',      'cycle_gan',\n",
    "    '--phase',      'test',\n",
    "    '--num_test',   '1',\n",
    "    '--load_size',  '256',\n",
    "    '--crop_size',  '256',\n",
    "]\n",
    "opt = TestOptions().parse()\n",
    "model = create_model(opt)\n",
    "model.setup(opt)\n",
    "model.eval()\n",
    "\n",
    "# ── 2) Preprocessing transform ───────────────────────────────────────────────\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# ── 3) Face detector ─────────────────────────────────────────────────────────\n",
    "mtcnn = MTCNN(keep_all=False)\n",
    "\n",
    "# ── 4) Start webcam ──────────────────────────────────────────────────────────\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    boxes, _ = mtcnn.detect(img_pil)\n",
    "    output = frame.copy()\n",
    "\n",
    "    if boxes is not None and len(boxes):\n",
    "        h, w = frame.shape[:2]\n",
    "        box = boxes[0]\n",
    "\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        face_w, face_h = x2 - x1, y2 - y1\n",
    "\n",
    "        # Increase crop region by percentage\n",
    "        expand_pct = 0.6  # 60% padding\n",
    "        pad_w = int(face_w * expand_pct / 2)\n",
    "        pad_h = int(face_h * expand_pct / 2)\n",
    "\n",
    "        x1_pad = max(0, x1 - pad_w)\n",
    "        y1_pad = max(0, y1 - pad_h)\n",
    "        x2_pad = min(w, x2 + pad_w)\n",
    "        y2_pad = min(h, y2 + pad_h)\n",
    "\n",
    "        # --- Stylize cropped face at high resolution ---\n",
    "        face_crop = img_pil.crop((x1_pad, y1_pad, x2_pad, y2_pad))\n",
    "        face_tensor = preprocess(face_crop).unsqueeze(0).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            face_fake = model.netG_A(face_tensor)\n",
    "\n",
    "        face_np = face_fake.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "        face_img = ((face_np + 1) / 2 * 255).clip(0, 255).astype(np.uint8)\n",
    "        face_bgr = cv2.cvtColor(face_img, cv2.COLOR_RGB2BGR)\n",
    "        target_w, target_h = x2_pad - x1_pad, y2_pad - y1_pad\n",
    "        face_bgr_resized = cv2.resize(face_bgr, (target_w, target_h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        # --- Gaussian blending mask ---\n",
    "        mask = np.zeros((target_h, target_w), dtype=np.float32)\n",
    "        cv2.circle(mask, (target_w // 2, target_h // 2), min(target_w, target_h) // 2, 1.0, -1)\n",
    "        mask = cv2.GaussianBlur(mask, (61, 61), 0)\n",
    "        mask = np.expand_dims(mask, axis=2)\n",
    "        mask = np.repeat(mask, 3, axis=2)\n",
    "\n",
    "        roi = output[y1_pad:y2_pad, x1_pad:x2_pad]\n",
    "\n",
    "        # --- Match brightness/color using LAB histogram alignment ---\n",
    "        if roi.shape == face_bgr_resized.shape:\n",
    "            face_lab = cv2.cvtColor(face_bgr_resized, cv2.COLOR_BGR2LAB).astype(np.float32)\n",
    "            roi_lab = cv2.cvtColor(roi, cv2.COLOR_BGR2LAB).astype(np.float32)\n",
    "\n",
    "            for c in range(3):\n",
    "                roi_mean, roi_std = roi_lab[:, :, c].mean(), roi_lab[:, :, c].std()\n",
    "                face_mean, face_std = face_lab[:, :, c].mean(), face_lab[:, :, c].std()\n",
    "                face_std = face_std if face_std > 1e-6 else 1.0\n",
    "                face_lab[:, :, c] = (face_lab[:, :, c] - face_mean) / face_std * roi_std + roi_mean\n",
    "                face_lab[:, :, c] = np.clip(face_lab[:, :, c], 0, 255)\n",
    "\n",
    "            face_bgr_resized = cv2.cvtColor(face_lab.astype(np.uint8), cv2.COLOR_LAB2BGR)\n",
    "\n",
    "        # --- Blend face and background ---\n",
    "        blended = (mask * face_bgr_resized + (1 - mask) * roi).astype(np.uint8)\n",
    "        output[y1_pad:y2_pad, x1_pad:x2_pad] = blended\n",
    "\n",
    "    cv2.imshow('Human 2 Arcane Composite', output)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1173a5a8-1a34-47a0-935a-5b34969079c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "             aspect_ratio: 1.0                           \n",
      "               batch_size: 1                             \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "                crop_size: 256                           \n",
      "                 dataroot: ./my_dataset                  \t[default: None]\n",
      "             dataset_mode: unaligned                     \n",
      "                direction: AtoB                          \n",
      "          display_winsize: 256                           \n",
      "                    epoch: 200                           \t[default: latest]\n",
      "                     eval: False                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: False                         \t[default: None]\n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 256                           \n",
      "         max_dataset_size: inf                           \n",
      "                    model: cycle_gan                     \t[default: test]\n",
      "               n_layers_D: 3                             \n",
      "                     name: human2arcane                  \t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "               no_dropout: True                          \n",
      "                  no_flip: False                         \n",
      "                     norm: instance                      \n",
      "                 num_test: 1                             \t[default: 50]\n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: test                          \n",
      "               preprocess: resize_and_crop               \n",
      "              results_dir: ./results/                    \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "                use_wandb: False                         \n",
      "                  verbose: False                         \n",
      "       wandb_project_name: CycleGAN-and-pix2pix          \n",
      "----------------- End -------------------\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [CycleGANModel] was created\n",
      "loading the model from ./checkpoints\\human2arcane\\200_net_G_A.pth\n",
      "loading the model from ./checkpoints\\human2arcane\\200_net_G_B.pth\n",
      "---------- Networks initialized -------------\n",
      "[Network G_A] Total number of parameters : 11.378 M\n",
      "[Network G_B] Total number of parameters : 11.378 M\n",
      "-----------------------------------------------\n",
      "Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "#with proper lighting best results so far are on actuall last epoch for corpped version. makes sense as my datasets are way havier than the example horse\n",
    "#zebra datasets and they took around 200 epochs to achive known resaults.\n",
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from options.test_options import TestOptions\n",
    "from models import create_model\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "# ── 1) Parse TestOptions ─────────────────────────────────────────────────────\n",
    "sys.argv = [\n",
    "    'test.py',\n",
    "    '--dataroot', './my_dataset',\n",
    "    '--name', 'human2arcane',\n",
    "    '--model', 'cycle_gan',\n",
    "    '--phase', 'test',\n",
    "    '--num_test', '1',\n",
    "    '--load_size', '256',\n",
    "    '--crop_size', '256',\n",
    "    '--epoch', '200',\n",
    "]\n",
    "opt = TestOptions().parse()\n",
    "model = create_model(opt)\n",
    "model.setup(opt)\n",
    "model.eval()\n",
    "\n",
    "# ── 2) Preprocessing transform ───────────────────────────────────────────────\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# ── 3) Face detector ─────────────────────────────────────────────────────────\n",
    "mtcnn = MTCNN(keep_all=False)\n",
    "\n",
    "# ── 4) Sliders ───────────────────────────────────────────────────────────────\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "cv2.namedWindow('Controls')\n",
    "cv2.createTrackbar('Contrast', 'Controls', 100, 300, nothing)   # 1.0–3.0\n",
    "cv2.createTrackbar('Brightness', 'Controls', 100, 200, nothing) # -1.0–1.0\n",
    "cv2.createTrackbar('Gamma', 'Controls', 100, 300, nothing)      # 0.1–3.0\n",
    "\n",
    "# ── 5) Start webcam ──────────────────────────────────────────────────────────\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    boxes, _ = mtcnn.detect(img_pil)\n",
    "    output = frame.copy()\n",
    "\n",
    "    # Read slider values\n",
    "    contrast_val = cv2.getTrackbarPos('Contrast', 'Controls') / 100.0  # 1.0–3.0\n",
    "    brightness_val = (cv2.getTrackbarPos('Brightness', 'Controls') - 100) / 100.0  # -1.0 to 1.0\n",
    "    gamma_val = cv2.getTrackbarPos('Gamma', 'Controls') / 100.0  # 0.1–3.0\n",
    "\n",
    "    if boxes is not None and len(boxes):\n",
    "        h, w = frame.shape[:2]\n",
    "        box = boxes[0]\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        face_w, face_h = x2 - x1, y2 - y1\n",
    "\n",
    "        expand_pct = 0.6\n",
    "        pad_w = int(face_w * expand_pct / 2)\n",
    "        pad_h = int(face_h * expand_pct / 2)\n",
    "\n",
    "        x1_pad = max(0, x1 - pad_w)\n",
    "        y1_pad = max(0, y1 - pad_h)\n",
    "        x2_pad = min(w, x2 + pad_w)\n",
    "        y2_pad = min(h, y2 + pad_h)\n",
    "\n",
    "        # Crop face and apply contrast/brightness/gamma BEFORE inference\n",
    "        face_crop = np.array(img_pil.crop((x1_pad, y1_pad, x2_pad, y2_pad))).astype(np.float32) / 255.0\n",
    "\n",
    "        # Contrast and brightness\n",
    "        face_crop = np.clip((face_crop - 0.5) * contrast_val + 0.5 + brightness_val, 0, 1)\n",
    "\n",
    "        # Gamma correction\n",
    "        face_crop = np.power(face_crop, 1.0 / max(gamma_val, 0.01))\n",
    "\n",
    "        # Back to PIL for transforms\n",
    "        face_crop = Image.fromarray((face_crop * 255).astype(np.uint8))\n",
    "\n",
    "        # Preprocess for model\n",
    "        face_tensor = preprocess(face_crop).unsqueeze(0).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            face_fake = model.netG_A(face_tensor)\n",
    "\n",
    "        face_np = face_fake.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "        face_img = ((face_np + 1) / 2 * 255).clip(0, 255).astype(np.uint8)\n",
    "        face_bgr = cv2.cvtColor(face_img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        target_w, target_h = x2_pad - x1_pad, y2_pad - y1_pad\n",
    "        face_bgr_resized = cv2.resize(face_bgr, (target_w, target_h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        # Gaussian blending mask\n",
    "        mask = np.zeros((target_h, target_w), dtype=np.float32)\n",
    "        cv2.circle(mask, (target_w // 2, target_h // 2), min(target_w, target_h) // 2, 1.0, -1)\n",
    "        mask = cv2.GaussianBlur(mask, (61, 61), 0)\n",
    "        mask = np.expand_dims(mask, axis=2)\n",
    "        mask = np.repeat(mask, 3, axis=2)\n",
    "\n",
    "        roi = output[y1_pad:y2_pad, x1_pad:x2_pad]\n",
    "\n",
    "        # Histogram alignment\n",
    "        if roi.shape == face_bgr_resized.shape:\n",
    "            face_lab = cv2.cvtColor(face_bgr_resized, cv2.COLOR_BGR2LAB).astype(np.float32)\n",
    "            roi_lab = cv2.cvtColor(roi, cv2.COLOR_BGR2LAB).astype(np.float32)\n",
    "\n",
    "            for c in range(3):\n",
    "                roi_mean, roi_std = roi_lab[:, :, c].mean(), roi_lab[:, :, c].std()\n",
    "                face_mean, face_std = face_lab[:, :, c].mean(), face_lab[:, :, c].std()\n",
    "                face_std = face_std if face_std > 1e-6 else 1.0\n",
    "                face_lab[:, :, c] = (face_lab[:, :, c] - face_mean) / face_std * roi_std + roi_mean\n",
    "                face_lab[:, :, c] = np.clip(face_lab[:, :, c], 0, 255)\n",
    "\n",
    "            face_bgr_resized = cv2.cvtColor(face_lab.astype(np.uint8), cv2.COLOR_LAB2BGR)\n",
    "\n",
    "        blended = (mask * face_bgr_resized + (1 - mask) * roi).astype(np.uint8)\n",
    "        output[y1_pad:y2_pad, x1_pad:x2_pad] = blended\n",
    "\n",
    "    cv2.imshow('Human 2 Arcane Composite', output)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b8350fb-f182-45c2-b641-ff4a8d85e6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "             aspect_ratio: 1.0                           \n",
      "               batch_size: 1                             \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "                crop_size: 256                           \n",
      "                 dataroot: ./my_dataset                  \t[default: None]\n",
      "             dataset_mode: unaligned                     \n",
      "                direction: AtoB                          \n",
      "          display_winsize: 256                           \n",
      "                    epoch: 200                           \t[default: latest]\n",
      "                     eval: False                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: False                         \t[default: None]\n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 256                           \n",
      "         max_dataset_size: inf                           \n",
      "                    model: cycle_gan                     \t[default: test]\n",
      "               n_layers_D: 3                             \n",
      "                     name: human2arcane                  \t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "               no_dropout: True                          \n",
      "                  no_flip: False                         \n",
      "                     norm: instance                      \n",
      "                 num_test: 1                             \t[default: 50]\n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: test                          \n",
      "               preprocess: resize_and_crop               \n",
      "              results_dir: ./results/                    \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "                use_wandb: False                         \n",
      "                  verbose: False                         \n",
      "       wandb_project_name: CycleGAN-and-pix2pix          \n",
      "----------------- End -------------------\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [CycleGANModel] was created\n",
      "loading the model from ./checkpoints\\human2arcane\\200_net_G_A.pth\n",
      "loading the model from ./checkpoints\\human2arcane\\200_net_G_B.pth\n",
      "---------- Networks initialized -------------\n",
      "[Network G_A] Total number of parameters : 11.378 M\n",
      "[Network G_B] Total number of parameters : 11.378 M\n",
      "-----------------------------------------------\n",
      "Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "## good results on epoch: 40,50 55 with high contrast,90!!!,110 wont look  out of place in the show\n",
    "#120 125 similar to last epoch less artifacts,130 looks good requires brightness adj,135 prty good some vanishing nose,\n",
    "#145 big eyes beard still present, 150 best yet, 155 best minor beard artifacts,170 works well no beard artifacts also with glasses\n",
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from options.test_options import TestOptions\n",
    "from models import create_model\n",
    "\n",
    "# ── 1) Parse TestOptions via sys.argv ─────────────────────────────────────────\n",
    "sys.argv = [\n",
    "    'test.py',\n",
    "    '--dataroot',   './my_dataset',\n",
    "    '--name',       'human2arcane',\n",
    "    '--model',      'cycle_gan',\n",
    "    '--phase',      'test',\n",
    "    '--num_test',   '1',\n",
    "    '--load_size',  '256',\n",
    "    '--crop_size',  '256',\n",
    "    '--epoch', '200',\n",
    "]\n",
    "opt = TestOptions().parse()\n",
    "model = create_model(opt)\n",
    "model.setup(opt)\n",
    "model.eval()\n",
    "\n",
    "# ── 2) Build full-frame preprocess transform ──────────────────────────────────\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize to model input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# ── 3) Slider setup ───────────────────────────────────────────────────────────\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "cv2.namedWindow('Controls')\n",
    "cv2.createTrackbar('Contrast', 'Controls', 100, 300, nothing)    # 1.0–3.0\n",
    "cv2.createTrackbar('Brightness', 'Controls', 100, 200, nothing)  # -1.0–1.0\n",
    "cv2.createTrackbar('Gamma', 'Controls', 100, 300, nothing)       # 0.1–3.0\n",
    "\n",
    "# ── 4) Start capture ──────────────────────────────────────────────────────────\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Get slider values\n",
    "    contrast_val = cv2.getTrackbarPos('Contrast', 'Controls') / 100.0\n",
    "    brightness_val = (cv2.getTrackbarPos('Brightness', 'Controls') - 100) / 100.0\n",
    "    gamma_val = max(cv2.getTrackbarPos('Gamma', 'Controls') / 100.0, 0.01)\n",
    "\n",
    "    # Resize input to 256x256 for processing\n",
    "    resized_input = cv2.resize(frame, (256, 256), interpolation=cv2.INTER_LINEAR)\n",
    "    input_rgb = cv2.cvtColor(resized_input, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "\n",
    "    # Apply contrast and brightness\n",
    "    adjusted = np.clip((input_rgb - 0.5) * contrast_val + 0.5 + brightness_val, 0, 1)\n",
    "\n",
    "    # Apply gamma correction\n",
    "    adjusted = np.power(adjusted, 1.0 / gamma_val)\n",
    "\n",
    "    # Convert to PIL for transform\n",
    "    adjusted_pil = Image.fromarray((adjusted * 255).astype(np.uint8))\n",
    "\n",
    "    # Preprocess for model\n",
    "    input_tensor = preprocess(adjusted_pil).unsqueeze(0).to(model.device)\n",
    "\n",
    "    # Stylize\n",
    "    with torch.no_grad():\n",
    "        fake = model.netG_A(input_tensor)\n",
    "\n",
    "    # Convert back to image\n",
    "    fake_np = fake.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "    fake_img = ((fake_np + 1) / 2 * 255).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    # Resize back to original frame size\n",
    "    output = cv2.cvtColor(fake_img, cv2.COLOR_RGB2BGR)\n",
    "    output = cv2.resize(output, (frame.shape[1], frame.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    cv2.imshow('Human 2 Arcane - Full Frame', output)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ef80d2-9953-4660-a2df-a6f1a9e757bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from options.test_options import TestOptions\n",
    "from models import create_model\n",
    "from facenet_pytorch import MTCNN\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ── 1) Parse TestOptions ─────────────────────────────────────────────────────\n",
    "sys.argv = [\n",
    "    'test.py',\n",
    "    '--dataroot', './my_dataset',\n",
    "    '--name', 'human2arcane',\n",
    "    '--model', 'cycle_gan',\n",
    "    '--phase', 'test',\n",
    "    '--num_test', '50',\n",
    "    '--load_size', '256',\n",
    "    '--crop_size', '256',\n",
    "    '--epoch', '200',\n",
    "]\n",
    "opt = TestOptions().parse()\n",
    "model = create_model(opt)\n",
    "model.setup(opt)\n",
    "model.eval()\n",
    "\n",
    "# ── 2) Preprocessing transform ───────────────────────────────────────────────\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# ── 3) Face detector ─────────────────────────────────────────────────────────\n",
    "mtcnn = MTCNN(keep_all=False)\n",
    "\n",
    "# ── 4) Inference on image folder ─────────────────────────────────────────────\n",
    "input_folder = './10imgtest/input/'\n",
    "output_folder = './10imgtest/output/'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if not filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        continue\n",
    "\n",
    "    image_path = os.path.join(input_folder, filename)\n",
    "    try:\n",
    "        img_pil_original = Image.open(image_path).convert('RGB')\n",
    "        img_np_original = np.array(img_pil_original)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading original image: {e}\")\n",
    "        continue\n",
    "\n",
    "    img_pil = img_pil_original.copy() # Create a copy for processing\n",
    "    boxes, _ = mtcnn.detect(img_pil)\n",
    "    output = np.array(img_pil)\n",
    "\n",
    "    if boxes is not None and len(boxes):\n",
    "        h, w = output.shape[:2]\n",
    "        box = boxes[0]\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        face_w, face_h = x2 - x1, y2 - y1\n",
    "\n",
    "        expand_pct = 0.4\n",
    "        pad_w = int(face_w * expand_pct / 2)\n",
    "        pad_h = int(face_h * expand_pct / 2)\n",
    "\n",
    "        x1_pad = max(0, x1 - pad_w)\n",
    "        y1_pad = max(0, y1 - pad_h)\n",
    "        x2_pad = min(w, x2 + pad_w)\n",
    "        y2_pad = min(h, y2 + pad_h)\n",
    "\n",
    "        face_crop = img_pil.crop((x1_pad, y1_pad, x2_pad, y2_pad))\n",
    "        face_tensor = preprocess(face_crop).unsqueeze(0).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            face_fake = model.netG_A(face_tensor)\n",
    "\n",
    "        face_np = face_fake.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "        face_img = ((face_np + 1) / 2 * 255).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "        # Resize and blend into original image\n",
    "        face_bgr_resized = cv2.resize(face_img, (x2_pad - x1_pad, y2_pad - y1_pad))\n",
    "        output[y1_pad:y2_pad, x1_pad:x2_pad] = face_bgr_resized\n",
    "\n",
    "    out_path = os.path.join(output_folder, filename)\n",
    "    cv2.imwrite(out_path, cv2.cvtColor(output, cv2.COLOR_RGB2BGR))\n",
    "    print(f\"Saved translated image: {out_path}\")\n",
    "\n",
    "    # Display images in the notebook\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img_np_original)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(output)  # Display the RGB 'output' directly\n",
    "    plt.title(\"Generated Image\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5330824-bb29-4cc5-82aa-fe6ae1818494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from options.test_options import TestOptions\n",
    "from models import create_model\n",
    "from facenet_pytorch import MTCNN\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "def perform_inference_single_epoch(image_path, epoch_to_test=100):\n",
    "    \"\"\"\n",
    "    Performs inference on a single image for a specified epoch.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "        epoch_to_test (int): The epoch number to use for the checkpoint.\n",
    "                             Defaults to 100.\n",
    "    \"\"\"\n",
    "    # Base argument list with the specific epoch\n",
    "    argv = [\n",
    "        'test.py',\n",
    "        '--dataroot', './my_dataset',  # Adjust if necessary\n",
    "        '--name', 'human2arcane',      # Your model name\n",
    "        '--model', 'cycle_gan',\n",
    "        '--phase', 'test',\n",
    "        '--num_test', '1',              # Only testing one image at a time\n",
    "        '--load_size', '256',\n",
    "        '--crop_size', '256',\n",
    "        '--epoch', str(epoch_to_test)\n",
    "    ]\n",
    "    sys.argv = argv\n",
    "    opt = TestOptions().parse()\n",
    "    opt.num_threads = 0\n",
    "    opt.batch_size = 1\n",
    "    opt.serial_batches = True\n",
    "\n",
    "    # ── 2) Preprocessing transform ───────────────────────────────────────────\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "\n",
    "    # ── 3) Face detector ─────────────────────────────────────────────────────\n",
    "    mtcnn = MTCNN(keep_all=False)\n",
    "\n",
    "    # Load the single input image\n",
    "    try:\n",
    "        img_pil_original = Image.open(image_path).convert('RGB')\n",
    "        img_np_original = np.array(img_pil_original)\n",
    "        filename = os.path.basename(image_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading original image: {e}\")\n",
    "        return\n",
    "\n",
    "    model = create_model(opt)\n",
    "    model.setup(opt)\n",
    "    model.eval()\n",
    "\n",
    "    img_pil = img_pil_original.copy()\n",
    "    boxes, _ = mtcnn.detect(img_pil)\n",
    "    output = np.array(img_pil)\n",
    "\n",
    "    if boxes is not None and len(boxes):\n",
    "        h, w = output.shape[:2]\n",
    "        box = boxes[0]\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        face_w, face_h = x2 - x1, y2 - y1\n",
    "\n",
    "        expand_pct = 0.4\n",
    "        pad_w = int(face_w * expand_pct / 2)\n",
    "        pad_h = int(face_h * expand_pct / 2)\n",
    "\n",
    "        x1_pad = max(0, x1 - pad_w)\n",
    "        y1_pad = max(0, y1 - pad_h)\n",
    "        x2_pad = min(w, x2 + pad_w)\n",
    "        y2_pad = min(h, y2 + pad_h)\n",
    "\n",
    "        face_crop = img_pil.crop((x1_pad, y1_pad, x2_pad, y2_pad))\n",
    "        face_tensor = preprocess(face_crop).unsqueeze(0).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            face_fake = model.netG_A(face_tensor)\n",
    "\n",
    "        face_np = face_fake.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "        face_img = ((face_np + 1) / 2 * 255).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "        face_bgr_resized = cv2.resize(face_img, (x2_pad - x1_pad, y2_pad - y1_pad))\n",
    "        output[y1_pad:y2_pad, x1_pad:x2_pad] = face_bgr_resized\n",
    "\n",
    "    print(\"Original Image:\")\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(img_np_original)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Generated Image (Epoch {epoch_to_test}):\")\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(output)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e03131-e2a0-432b-be88-42b825051602",
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_inference_single_epoch('./10imgtest/input/unnamed.png', epoch_to_test=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf432d3-a073-49c2-8508-2416ebf09522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54805b83-d1c0-4399-97cc-994b5c19caae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
